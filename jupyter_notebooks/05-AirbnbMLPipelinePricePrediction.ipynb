{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Have the best model set up for the ML.  \n",
    "\n",
    "## Inputs\n",
    "\n",
    "* Use the data processed that was collected from Kaggle `outputs/datasets/collection/AirbnbEuropeanCities.csv`. \n",
    "\n",
    "## Outputs\n",
    "\n",
    "* Have the ML set up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have the working directory changed from its current folder to its parent folder.\n",
    "* We access the current directory with `os.getcwd()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have the parent of the current directory set up as the new current directory.\n",
    "* `os.path.dirname()` gets the parent directory;\n",
    "* `os.chir()` defines the new current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have the new current directory confirmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Have data loaded for the ML process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(f\"outputs/datasets/collection/EuropeanCitiesAirbnb.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Have data filtered according to the analysis requirements to set up the ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_of_interest = ['Amsterdam', 'Barcelona', 'London']\n",
    "df_filtered = df[(df['city'].isin(cities_of_interest)) & (df['bedrooms'] <= 3) & (df['room_type'] == 'Entire home/apt') & (df['daily_price'] <= 750)]\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Have variables that won't be used dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import PowerTransformer, OneHotEncoder, FunctionTransformer\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "df_ml = df_filtered.drop(labels=['room_type', 'weekends'], axis=1)\n",
    "print(df_ml.shape)\n",
    "df_ml.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Visualising the Original Distribution of `daily_price`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Visualizing the distribution of the original daily_price\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(df_ml['daily_price'], bins=50, color='blue', alpha=0.7)\n",
    "plt.title('Original Distribution of Daily Price')\n",
    "plt.xlabel('Daily Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Function\n",
    "Have a preprocessing pipeline created to apply the necessary transformations to the dataframe:\n",
    "  + One-Hot Encoding: For the categorical feature `city`;\n",
    "  + Box-Cox Transformation: For numerical feature `city_center_dist_km`;\n",
    "  + Logarithmic (Log_e) Transformation: For numerical feature `metro_dist_km`; \n",
    "  + Passthrough: For features that don’t require transformation, like `bedrooms`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data():\n",
    "    # Define the numerical and categorical features\n",
    "    categorical_features = ['city']\n",
    "    numerical_features_boxcox = ['city_center_dist_km']  \n",
    "    numerical_features_log = ['metro_dist_km']      \n",
    "        \n",
    "    # One-hot encoder for city\n",
    "    categorical_transformer = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "    # Define transformers for numerical and categorical features\n",
    "    numerical_transformer_boxcox = PowerTransformer(method='box-cox')      \n",
    "    numerical_transformer_log = FunctionTransformer(np.log1p, validate=True)     \n",
    "    \n",
    "    # Combine the transformers into a ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            # ('num_boxcox', numerical_transformer_boxcox, numerical_features_boxcox),            \n",
    "            ('cat', categorical_transformer, categorical_features),\n",
    "            ('num_boxcox', numerical_transformer_boxcox, numerical_features_boxcox),\n",
    "            ('num_log', numerical_transformer_log, numerical_features_log),            \n",
    "        ],\n",
    "        remainder='passthrough'  \n",
    "    )\n",
    "    \n",
    "    return preprocessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining and Training  ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_best_pipeline(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Function to check multiple models with preprocessing and hyperparameter tuning.\n",
    "    Returns the best model and results for each.\n",
    "    \"\"\"\n",
    "    # Define the models to test\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(),\n",
    "        'Lasso Regression': Lasso(),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(random_state=42)\n",
    "    }\n",
    "\n",
    "    # Define the hyperparameter grids for each model\n",
    "    param_grids = {\n",
    "        'Linear Regression': {},  \n",
    "        'Ridge Regression': {'model__alpha': [0.1, 1.0, 10]},\n",
    "        'Lasso Regression': {'model__alpha': [0.01, 0.1, 1.0]},\n",
    "        'Gradient Boosting': {\n",
    "            'model__n_estimators': [100, 200, 300],\n",
    "            'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "            'model__max_depth': [3, 5, 7]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    best_pipelines = {}  \n",
    "\n",
    "    # Loop through each model\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Running {model_name} with hyperparameter tuning...\")\n",
    "        \n",
    "        # Create a pipeline for each model with preprocessing\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocess_data()),  \n",
    "            ('model', model)  \n",
    "        ])\n",
    "        \n",
    "        # Use GridSearchCV for hyperparameter tuning\n",
    "        grid_search = GridSearchCV(pipeline, param_grids[model_name], cv=5, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "        \n",
    "        # Fit the model with the best hyperparameters\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Get the best model pipeline\n",
    "        best_pipeline = grid_search.best_estimator_\n",
    "        best_pipelines[model_name] = best_pipeline\n",
    "        \n",
    "        # Make predictions on the test set\n",
    "        y_pred_log = best_pipeline.predict(X_test)\n",
    "        y_pred = np.expm1(y_pred_log)  # Inverse log transformation\n",
    "        \n",
    "        # Evaluate the model\n",
    "        mae = mean_absolute_error(np.expm1(y_test), y_pred)\n",
    "        mse = mean_squared_error(np.expm1(y_test), y_pred)\n",
    "        r2 = r2_score(np.expm1(y_test), y_pred)\n",
    "        \n",
    "        # Store the results, including the best hyperparameters\n",
    "        results[model_name] = {\n",
    "            'Best Hyperparameters': grid_search.best_params_,\n",
    "            'Mean Absolute Error': mae,\n",
    "            'Mean Squared Error': mse,\n",
    "            'R² Score': r2\n",
    "        }\n",
    "\n",
    "    return results, best_pipelines  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have the features (X) and the target variable (y) defined, and the log transformation applied to the target (daily_price) to help normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_ml[['city', 'bedrooms', 'city_center_dist_km', 'metro_dist_km']]\n",
    "y = np.log1p(df_ml['daily_price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the Log-Transformed `daily_price`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml['log_daily_price'] = np.log1p(df_ml['daily_price'])\n",
    "\n",
    "# Visualizing the distribution of log-transformed daily_price\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(df_ml['log_daily_price'], bins=50, color='green', alpha=0.7)\n",
    "plt.title('Log-Transformed Distribution of Daily Price')\n",
    "plt.xlabel('Log of Daily Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have the dataframe split into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Model and Check the Best Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have the `check_best_pipeline()` function run to check which model performs the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function to try multiple models with hyperparameter tuning\n",
    "results, best_pipelines = check_best_pipeline(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Display the results for each model\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"Results for {model_name}:\")\n",
    "    print(f\"Best Hyperparameters: {metrics['Best Hyperparameters']}\")\n",
    "    print(f\"Mean Absolute Error: {metrics['Mean Absolute Error']}\")\n",
    "    print(f\"Mean Squared Error: {metrics['Mean Squared Error']}\")\n",
    "    print(f\"R² Score: {metrics['R² Score']}\")\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gb_pipeline = best_pipelines.get('Gradient Boosting')\n",
    "\n",
    "if best_gb_pipeline:\n",
    "    print(\"Accessing the best Gradient Boosting pipeline...\")\n",
    "\n",
    "    # Access the fitted Gradient Boosting model inside the pipeline\n",
    "    best_gb_model = best_gb_pipeline.named_steps['model']\n",
    "    \n",
    "    # Check feature importance\n",
    "    importances = best_gb_model.feature_importances_\n",
    "    \n",
    "    # Get the preprocessor\n",
    "    preprocessor = best_gb_pipeline.named_steps['preprocessor']\n",
    "    \n",
    "    # Print structure to debug the transformers\n",
    "    print(\"Preprocessor structure:\", preprocessor)\n",
    "\n",
    "    try:\n",
    "        # Access the OneHotEncoder from the 'cat' transformer\n",
    "        one_hot_encoder = preprocessor.named_transformers_['cat']  # This is the OneHotEncoder for 'city'\n",
    "        \n",
    "        # Try to use get_feature_names_out(), fallback to get_feature_names()\n",
    "        try:\n",
    "            categorical_columns = one_hot_encoder.get_feature_names_out(['city'])\n",
    "        except AttributeError:\n",
    "            categorical_columns = one_hot_encoder.get_feature_names(['city'])  \n",
    "\n",
    "        # Combine the numerical feature names manually\n",
    "        numerical_features = ['bedrooms', 'city_center_dist_km', 'metro_dist_km']\n",
    "        feature_names = numerical_features + list(categorical_columns)\n",
    "\n",
    "        # Debugging: Check the lengths of feature_names and importances\n",
    "        print(\"Number of features (from feature_names):\", len(feature_names))\n",
    "        print(\"Number of importances (from model):\", len(importances))\n",
    "\n",
    "        # Ensure the lengths match\n",
    "        if len(feature_names) == len(importances):\n",
    "            # Plot the feature importance\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.barh(feature_names, importances)\n",
    "            plt.title('Feature Importance - Gradient Boosting')\n",
    "            plt.xlabel('Importance Score')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Error: The number of feature names and importances do not match.\")\n",
    "\n",
    "    except AttributeError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Ensure you're accessing the correct OneHotEncoder in the pipeline.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize the Gradient Boosting Model (best model from hyperparameter tuning)\n",
    "best_gb_pipeline = best_pipelines.get('Gradient Boosting')\n",
    "\n",
    "if best_gb_pipeline:\n",
    "    # Access the fitted Gradient Boosting model inside the pipeline\n",
    "    best_gb_model = best_gb_pipeline.named_steps['model']\n",
    "\n",
    "    # Feature Importance Analysis\n",
    "    importances = best_gb_model.feature_importances_\n",
    "\n",
    "    # Get the preprocessor\n",
    "    preprocessor = best_gb_pipeline.named_steps['preprocessor']\n",
    "\n",
    "    try:\n",
    "        # Access the OneHotEncoder from the 'cat' transformer\n",
    "        one_hot_encoder = preprocessor.named_transformers_['cat']  # This is the OneHotEncoder for 'city'\n",
    "        \n",
    "        # Try to use get_feature_names_out(), fallback to get_feature_names()\n",
    "        try:\n",
    "            categorical_columns = one_hot_encoder.get_feature_names_out(['city'])\n",
    "        except AttributeError:\n",
    "            categorical_columns = one_hot_encoder.get_feature_names(['city'])  # Fallback for older versions of scikit-learn\n",
    "\n",
    "        # Combine the numerical feature names manually\n",
    "        numerical_features = ['bedrooms', 'city_center_dist_km', 'metro_dist_km']\n",
    "        feature_names = numerical_features + list(categorical_columns)\n",
    "\n",
    "        # Debugging: Check the lengths of feature_names and importances\n",
    "        print(\"Number of features (from feature_names):\", len(feature_names))\n",
    "        print(\"Number of importances (from model):\", len(importances))\n",
    "\n",
    "        # Ensure the lengths match\n",
    "        if len(feature_names) == len(importances):\n",
    "            # Plot the feature importance\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.barh(feature_names, importances)\n",
    "            plt.title('Feature Importance - Gradient Boosting')\n",
    "            plt.xlabel('Importance Score')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Error: The number of feature names and importances do not match.\")\n",
    "\n",
    "        # Make final predictions using the best Gradient Boosting pipeline\n",
    "        y_pred_log = best_gb_pipeline.predict(X_test)\n",
    "        y_pred = np.expm1(y_pred_log)  # Inverse log transform the predictions\n",
    "\n",
    "        # Evaluate the final model (no need to inverse log transform y_test again)\n",
    "        mae = mean_absolute_error(np.expm1(y_test), y_pred)\n",
    "        mse = mean_squared_error(np.expm1(y_test), y_pred)\n",
    "        r2 = r2_score(np.expm1(y_test), y_pred)\n",
    "\n",
    "        print(f\"Mean Absolute Error (Final Model): {mae}\")\n",
    "        print(f\"Mean Squared Error (Final Model): {mse}\")\n",
    "        print(f\"R² Score (Final Model): {r2}\")\n",
    "\n",
    "    except AttributeError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Ensure you're accessing the correct OneHotEncoder in the pipeline.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushing File to Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split your data into training and test sets\n",
    "X = df_ml[['city', 'bedrooms', 'city_center_dist_km', 'metro_dist_km']]  # Features\n",
    "y = np.log1p(df_ml['daily_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Specify version and file path\n",
    "version = 'v1'\n",
    "file_path = f'outputs/ml_pipeline/airbnb_price_prediction/{version}'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "try:\n",
    "    os.makedirs(name=file_path)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training and test sets as CSV files\n",
    "X_train.to_csv(f\"{file_path}/X_train.csv\", index=False)\n",
    "X_test.to_csv(f\"{file_path}/X_test.csv\", index=False)\n",
    "y_train.to_csv(f\"{file_path}/y_train.csv\", index=False)\n",
    "y_test.to_csv(f\"{file_path}/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(value=best_gb_pipeline, filename=f\"{file_path}/price_prediction_pipeline.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml.to_csv(f\"{file_path}/TrainSet.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the feature importance plot\n",
    "importances = best_gb_model.feature_importances_\n",
    "\n",
    "# Get the preprocessor from the pipeline\n",
    "preprocessor = best_gb_pipeline.named_steps['preprocessor']\n",
    "\n",
    "# Try to retrieve feature names for the categorical columns (city) from OneHotEncoder\n",
    "try:\n",
    "    one_hot_encoder = preprocessor.named_transformers_['cat']  # Access the OneHotEncoder for 'city'\n",
    "    \n",
    "    # Try to use get_feature_names_out(), fallback to get_feature_names()\n",
    "    try:\n",
    "        city_feature_names = one_hot_encoder.get_feature_names_out(['city'])\n",
    "    except AttributeError:\n",
    "        city_feature_names = one_hot_encoder.get_feature_names(['city'])  # For older versions of scikit-learn\n",
    "\n",
    "    # Combine the feature names for numerical and categorical columns\n",
    "    feature_names = ['bedrooms', 'city_center_dist_km', 'metro_dist_km'] + list(city_feature_names)\n",
    "\n",
    "    # Plot the feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_names, importances)\n",
    "    plt.title('Feature Importance - Gradient Boosting')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "except AttributeError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Ensure you're accessing the correct OneHotEncoder in the pipeline.\")\n",
    "\n",
    "# Save the performance metrics (MAE, MSE, R²)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the performance metrics (MAE, MSE, R²)\n",
    "mae = mean_absolute_error(np.expm1(y_test), y_pred)\n",
    "mse = mean_squared_error(np.expm1(y_test), y_pred)\n",
    "r2 = r2_score(np.expm1(y_test), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results in a CSV\n",
    "with open(f\"{file_path}/model_performance.csv\", 'w') as f:\n",
    "    f.write('Metric,Value\\n')\n",
    "    f.write(f'Mean Absolute Error,{mae}\\n')\n",
    "    f.write(f'Mean Squared Error,{mse}\\n')\n",
    "    f.write(f'R² Score,{r2}\\n')\n",
    "\n",
    "print(\"Files saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
